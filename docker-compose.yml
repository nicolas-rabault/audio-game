# See NOTE comments for places to modify.
services:
  traefik:
    image: traefik:v3.3.1
    command:
      # Swarm provider configuration
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"

      # This is set up for HTTP. If you want HTTPS support for production, use Docker Swarm
      # (check out swarm-deploy.yml) or ask ChatGPT to modify this file for you.
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      # Redirect all HTTP traffic to HTTPS
      - "--entrypoints.web.http.redirections.entryPoint.to=websecure"
      - "--entrypoints.web.http.redirections.entryPoint.scheme=https"
      # Load static TLS certs (mkcert) from file provider
      - "--providers.file.filename=/etc/traefik/dynamic.yml"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "./traefik/dynamic.yml:/etc/traefik/dynamic.yml:ro"
      - "./certs:/certs:ro"

  frontend:
    image: unmute-frontend:latest
    build:
      context: frontend/
      dockerfile: hot-reloading.Dockerfile
    volumes:
      - ./frontend/src:/app/src
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=PathPrefix(`/`)"
      - "traefik.http.routers.frontend.entrypoints=websecure"
      - "traefik.http.routers.frontend.tls=true"
      - "traefik.http.services.frontend.loadbalancer.server.port=3000"
      - "traefik.http.routers.frontend.priority=10" # lowest priority

  backend:
    image: unmute-backend:latest
    build:
      context: ./
      target: hot-reloading
    volumes:
      - ./unmute:/app/unmute
    environment:
      # RTX 50 SERIES FIX: Point to native services running outside Docker
      # Use host.docker.internal to access services on the host machine
      - KYUTAI_STT_URL=ws://host.docker.internal:8090
      - KYUTAI_TTS_URL=ws://host.docker.internal:8089
      - KYUTAI_VOICE_CLONING_URL=http://host.docker.internal:8092
      # Point LLM to OpenAI's API
      - KYUTAI_LLM_URL=https://api.openai.com
      # Choose your OpenAI model
      - KYUTAI_LLM_MODEL=gpt-4.1
      # Pass through your OpenAI API key from host env
      - KYUTAI_LLM_API_KEY=$OPENAI_API_KEY
      - NEWSAPI_API_KEY=$NEWSAPI_API_KEY
    extra_hosts:
      # RTX 50 SERIES FIX: Enable communication with host services
      - "host.docker.internal:host-gateway"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.backend.rule=PathPrefix(`/api`)"
      - "traefik.http.routers.backend.middlewares=strip-api"
      - "traefik.http.middlewares.strip-api.replacepathregex.regex=^/api/(.*)"
      - "traefik.http.middlewares.strip-api.replacepathregex.replacement=/$$1"
      - "traefik.http.routers.backend.entrypoints=websecure"
      - "traefik.http.routers.backend.tls=true"
      - "traefik.http.services.backend.loadbalancer.server.port=80"
      - "traefik.http.routers.backend.priority=100" # higher priority than frontend
      - "prometheus-port=80"
  # RTX 50 SERIES FIX: TTS and STT services disabled in Docker
  # These services must be run natively using the dockerless scripts
  # See: ./dockerless/start_tts.sh and ./dockerless/start_stt.sh
  # Reason: Docker images use CUDA 12.1 which doesn't support RTX 50 series (Blackwell)
  # Running natively uses your system's CUDA 12.9 which has full RTX 50 support

  # tts:
  #   image: moshi-server:latest
  #   command: ["worker", "--config", "configs/tts.toml"]
  #   build:
  #     context: services/moshi-server
  #     dockerfile: public.Dockerfile
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
  #   volumes:
  #     - ./volumes/hf-cache:/root/.cache/huggingface
  #     - ./volumes/cargo-registry-tts:/root/.cargo/registry
  #     - ./volumes/tts-target:/app/target
  #     - ./volumes/uv-cache:/root/.cache/uv
  #     - /tmp/models/:/models
  #     - ./volumes/tts-logs:/logs
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # stt:
  #   image: moshi-server:latest
  #   command: ["worker", "--config", "configs/stt.toml"]
  #   build:
  #     context: services/moshi-server
  #     dockerfile: public.Dockerfile
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
  #   volumes:
  #     - ./volumes/hf-cache:/root/.cache/huggingface
  #     - ./volumes/cargo-registry-stt:/root/.cargo/registry
  #     - ./volumes/stt-target:/app/target
  #     - ./volumes/uv-cache:/root/.cache/uv
  #     - /tmp/models/:/models
  #     - ./volumes/stt-logs:/logs
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # llm:
  #   # Disabled when using OpenAI API directly
  #   image: vllm/vllm-openai:v0.9.2
  #   command:
  #     [
  #       "--model=meta-llama/Llama-3.2-1B-Instruct",
  #       "--max-model-len=1536",
  #       "--dtype=bfloat16",
  #       "--gpu-memory-utilization=0.4"
  #     ]
  #   volumes:
  #     - ./volumes/hf-cache:/root/.cache/huggingface
  #     - ./volumes/vllm-cache:/root/.cache/vllm
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [ gpu ]

networks:
  default:
